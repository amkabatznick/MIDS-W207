{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you'll work with text data from newsgroup postings on a variety of topics. You'll train classifiers to distinguish between the topics based on the text of the posts. Whereas with digit classification, the input is relatively dense: a 28x28 matrix of pixels, many of which are non-zero, here we'll represent each document with a \"bag-of-words\" model. As you'll see, this makes the feature representation quite sparse -- only a few words of the total vocabulary are active in any given document. The bag-of-words assumption here is that the label depends only on the words; their order is not important.\n",
    "\n",
    "The SK-learn documentation on feature extraction will prove useful:\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "Each problem can be addressed succinctly with the included packages -- please don't add any more. Grading will be based on writing clean, commented code, along with a few short answers.\n",
    "\n",
    "As always, you're welcome to work on the project in groups and discuss ideas on the course wall, but please prepare your own write-up and write your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, stripping out metadata so that we learn classifiers that only use textual features. By default, newsgroups data is split into train and test sets. We further split the test so we have a dev set. Note that we specify 4 categories to use for this project. If you remove the categories argument from the fetch function, you'll get all 20 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (2034,)\n",
      "test label shape: (677,)\n",
      "dev label shape: (676,)\n",
      "labels names: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "num_test = len(newsgroups_test.target)\n",
    "test_data, test_labels = newsgroups_test.data[num_test/2:], newsgroups_test.target[num_test/2:]\n",
    "dev_data, dev_labels = newsgroups_test.data[:num_test/2], newsgroups_test.target[:num_test/2]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print 'training label shape:', train_labels.shape\n",
    "print 'test label shape:', test_labels.shape\n",
    "print 'dev label shape:', dev_labels.shape\n",
    "print 'labels names:', newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) For each of the first 5 training examples, print the text of the message along with the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[4mExample 1\u001b[0m\n",
      "\u001b[1mLabel: comp.graphics\u001b[0m\n",
      "\u001b[1mMessage: \u001b[0m\n",
      "\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "\u001b[1m\u001b[4mExample 2\u001b[0m\n",
      "\u001b[1mLabel: talk.religion.misc\u001b[0m\n",
      "\u001b[1mMessage: \u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries.\n",
      "\n",
      "\u001b[1m\u001b[4mExample 3\u001b[0m\n",
      "\u001b[1mLabel: sci.space\u001b[0m\n",
      "\u001b[1mMessage: \u001b[0m\n",
      "\n",
      "\n",
      " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
      "\n",
      "MB>                                                             So the\n",
      "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
      "\n",
      "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
      "\n",
      "Couldn't we just say periapsis or apoapsis?\n",
      "\n",
      " \n",
      "\n",
      "\u001b[1m\u001b[4mExample 4\u001b[0m\n",
      "\u001b[1mLabel: alt.atheism\u001b[0m\n",
      "\u001b[1mMessage: \u001b[0m\n",
      "\n",
      "I have a request for those who would like to see Charley Wingate\n",
      "respond to the \"Charley Challenges\" (and judging from my e-mail, there\n",
      "appear to be quite a few of you.)  \n",
      "\n",
      "It is clear that Mr. Wingate intends to continue to post tangential or\n",
      "unrelated articles while ingoring the Challenges themselves.  Between\n",
      "the last two re-postings of the Challenges, I noted perhaps a dozen or\n",
      "more posts by Mr. Wingate, none of which answered a single Challenge.  \n",
      "\n",
      "It seems unmistakable to me that Mr. Wingate hopes that the questions\n",
      "will just go away, and he is doing his level best to change the\n",
      "subject.  Given that this seems a rather common net.theist tactic, I\n",
      "would like to suggest that we impress upon him our desire for answers,\n",
      "in the following manner:\n",
      "\n",
      "1. Ignore any future articles by Mr. Wingate that do not address the\n",
      "Challenges, until he answers them or explictly announces that he\n",
      "refuses to do so.\n",
      "\n",
      "--or--\n",
      "\n",
      "2. If you must respond to one of his articles, include within it\n",
      "something similar to the following:\n",
      "\n",
      "    \"Please answer the questions posed to you in the Charley Challenges.\"\n",
      "\n",
      "Really, I'm not looking to humiliate anyone here, I just want some\n",
      "honest answers.  You wouldn't think that honesty would be too much to\n",
      "ask from a devout Christian, would you?  \n",
      "\n",
      "Nevermind, that was a rhetorical question.\n",
      "\n",
      "\u001b[1m\u001b[4mExample 5\u001b[0m\n",
      "\u001b[1mLabel: sci.space\u001b[0m\n",
      "\u001b[1mMessage: \u001b[0m\n",
      "\n",
      "AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
      "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
      "\n",
      "Does anyone know more about this?  How much, to attend????\n",
      "\n",
      "Anyone want to go?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def P1(num_examples=5):\n",
    "    ### STUDENT START ###\n",
    "    #Loop through the subset of training examples\n",
    "    for i in range(num_examples):\n",
    "        print('\\033[1m\\033[4mExample {0:0.0f}\\033[0m'.format(i+1))\n",
    "        print('\\033[1mLabel: '+newsgroups_train.target_names[train_labels[i]]+\"\\033[0m\")\n",
    "        print('\\033[1mMessage: \\033[0m\\n')\n",
    "        print(train_data[i]+\"\\n\")\n",
    "    ### STUDENT END ###\n",
    "P1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Use CountVectorizer to turn the raw training text into feature vectors. You should use the fit_transform function, which makes 2 passes through the data: first it computes the vocabulary (\"fit\"), second it converts the raw text into feature vectors using the vocabulary (\"transform\").\n",
    "\n",
    "#### The vectorizer has a lot of options. To get familiar with some of them, write code to answer these questions:\n",
    "\n",
    "#### a. The output of the transform (also of fit_transform) is a sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html. What is the size of the vocabulary? What is the average number of non-zero features per example? What fraction of the entries in the matrix are non-zero? Hint: use \"nnz\" and \"shape\" attributes.\n",
    "\n",
    "#### b. What are the 0th and last feature strings (in alphabetical order)? Hint: use the vectorizer's get_feature_names function.\n",
    "\n",
    "#### c. Specify your own vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"]. Confirm the training vectors are appropriately shaped. Now what's the average number of non-zero features per example?\n",
    "\n",
    "#### d. Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\" to extract bigram and trigram character features. What size vocabulary does this yield?\n",
    "\n",
    "#### e. Use the \"min_df\" argument to prune words that appear in fewer than 10 documents. What size vocabulary does this yield?\n",
    "\n",
    "#### f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary? Hint: build a vocabulary for both train and dev and look at the size of the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[4mQuestion 2a.\u001b[0m\n",
      "The size of the vocabulary is 26,879\n",
      "The average number of non-zero features per example is 96\n",
      "The fraction of entries in the matrix that are non-zero is 0.36%\n",
      "\n",
      "\u001b[1m\u001b[4mQuestion 2b.\u001b[0m\n",
      "The first feature string is \"00\"\n",
      "The last feature string is \"zyxel\"\n",
      "\n",
      "\u001b[1m\u001b[4mQuestion 2c.\u001b[0m\n",
      "The shape of the training data 2034 by 4, which matches expecations\n",
      "The average number of non-zero features per example for the data trained on the select vocab list is 0.27\n",
      "\n",
      "\u001b[1m\u001b[4mQuestion 2d.\u001b[0m\n",
      "The size of the vocabulary for the bigram character features is 3,291\n",
      "The size of the vocabulary for the trigram character features is 32,187\n",
      "\n",
      "\u001b[1m\u001b[4mQuestion 2e.\u001b[0m\n",
      "The size of the vocabulary after pruning words that appear in fewer than 10 documents is 3,064\n",
      "\n",
      "\u001b[1m\u001b[4mQuestion 2f.\u001b[0m\n",
      "There are 4,027 words in the dev data that are missing from the vocabulary built from the training data\n",
      "This means that the missing words are 15% the length of the training data vocabulary\n"
     ]
    }
   ],
   "source": [
    "def P2():\n",
    "    ### STUDENT START ###\n",
    "    #Initializes the vectorizer\n",
    "    CountVec = CountVectorizer()\n",
    "    #Transforms the the dataset to a sparse matrix where each word is represented by a column\n",
    "    vec_train_data = CountVec.fit_transform(train_data)\n",
    "    print('\\033[1m\\033[4mQuestion 2a.\\033[0m')\n",
    "    #Gets the number of columns from the transformed data\n",
    "    print('The size of the vocabulary is {0:,.0f}'.format(vec_train_data.shape[1]))\n",
    "    #Uses the NNZ function (Number of non-zeros) divided by the number of rows to determine average words per row\n",
    "    print('The average number of non-zero features per example is {0:,.0f}'.format(vec_train_data.nnz/\n",
    "                                                                                    vec_train_data.shape[0]))\n",
    "    #Uses the NNZ function (Number of non-zeros) divided by the number of rows to determine average words per row\n",
    "    print('The fraction of entries in the matrix that are non-zero is {0:.2f}%'.format((vec_train_data.nnz*1.0\n",
    "                                                                                        /(vec_train_data.shape[0]*\n",
    "                                                                                          vec_train_data.shape[1]))\n",
    "                                                                                       *100))\n",
    "\n",
    "    print('\\n\\033[1m\\033[4mQuestion 2b.\\033[0m')\n",
    "    #Uses the get_feature_names from the CountVectorizer to find the feature strings at each index specified\n",
    "    print('The first feature string is \"{}\"'.format(CountVec.get_feature_names()[0]))\n",
    "    print('The last feature string is \"{}\"'.format(CountVec.get_feature_names()[-1]))\n",
    "\n",
    "    #Trains the Count Vectorizer using only the sepcified vocab words\n",
    "    vocab = [\"atheism\", \"graphics\", \"space\", \"religion\"]\n",
    "    CountVecVocab = CountVectorizer(vocabulary=vocab)\n",
    "    vec_train_data_2 = CountVecVocab.fit_transform(train_data)\n",
    "    print('\\n\\033[1m\\033[4mQuestion 2c.\\033[0m')\n",
    "    #Checks the shape of this new data confirms the expected size\n",
    "    print('The shape of the training data {} by {}, which matches expecations'.format(vec_train_data_2.shape[0],\n",
    "                                                                                vec_train_data_2.shape[1]))\n",
    "    #Look at average number of non-zero features per row\n",
    "    print('The average number of non-zero features per example for the data trained on the select vocab list is {0:,.2f}'\n",
    "          \n",
    "          .format(float(vec_train_data_2.nnz)/vec_train_data_2.shape[0]))\n",
    "\n",
    "\n",
    "    print('\\n\\033[1m\\033[4mQuestion 2d.\\033[0m')\n",
    "    #Use the CountVectorizer for bigram charecters\n",
    "    CountVecVocab2d = CountVectorizer(analyzer='char',ngram_range=(2,2))\n",
    "    vec_train_data_2d = CountVecVocab2d.fit_transform(train_data)\n",
    "    print('The size of the vocabulary for the bigram character features is {0:,.0f}'\n",
    "          .format(vec_train_data_2d.shape[1]))\n",
    "    #Use the CountVectorizer for trigram charecters\n",
    "    CountVecVocab3d = CountVectorizer(analyzer='char',ngram_range=(3,3))\n",
    "    vec_train_data_3d = CountVecVocab3d.fit_transform(train_data)\n",
    "    print('The size of the vocabulary for the trigram character features is {0:,.0f}'\n",
    "          .format(vec_train_data_3d.shape[1]))\n",
    "\n",
    "    print('\\n\\033[1m\\033[4mQuestion 2e.\\033[0m')\n",
    "    #Trims words that appear fewer than 10x\n",
    "    CountVecPrun = CountVectorizer(min_df=10)\n",
    "    train_data_prun = CountVecPrun.fit_transform(train_data)\n",
    "    print('The size of the vocabulary after pruning words that appear in fewer than 10 documents is {0:,.0f}'\n",
    "          .format(train_data_prun.shape[1]))\n",
    "\n",
    "    print('\\n\\033[1m\\033[4mQuestion 2f.\\033[0m')\n",
    "    #Intializes a CountVectorizer for Dev_Data\n",
    "    CountVecDev = CountVectorizer()\n",
    "    #Transforms the the dataset to a sparse matrix where each word is represented by a column, here we use the dev_data\n",
    "    vec_train_data = CountVecDev.fit_transform(dev_data)\n",
    "    #Finds the variables in the dev data not in the training data\n",
    "    DevNotInTrain = len(set(CountVecDev.get_feature_names()).difference(set(CountVec.get_feature_names()))) \n",
    "    print('There are {0:,.0f} words in the dev data that are missing from the vocabulary built from the training data'\n",
    "          .format(DevNotInTrain))\n",
    "    print('This means that the missing words are {0:0.0f}% the length of the training data vocabulary'\n",
    "          .format((float(DevNotInTrain)/len(set(CountVec.get_feature_names())))*100))\n",
    "    ### STUDENT END ###\n",
    "    \n",
    "P2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Use the default CountVectorizer options and report the f1 score (use metrics.f1_score) for a k nearest neighbors classifier; find the optimal value for k. Also fit a Multinomial Naive Bayes model and find the optimal value for alpha. Finally, fit a logistic regression model and find the optimal value for the regularization strength C using l2 regularization. A few questions:\n",
    "\n",
    "#### a. Why doesn't nearest neighbors work well for this problem?\n",
    "\n",
    "#### b. Any ideas why logistic regression doesn't work as well as Naive Bayes?\n",
    "\n",
    "#### c. Logistic regression estimates a weight vector for each class, which you can access with the coef\\_ attribute. Output the sum of the squared weight values for each class for each setting of the C parameter. Briefly explain the relationship between the sum and the value of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[4mK-Nearest Neighbors\u001b[0m\n",
      "The best choice for k is 97, with a f1_score of 46.4%\n",
      "\n",
      "\u001b[1m\u001b[4mMultinomial Naive Bayes\u001b[0m\n",
      "The best choice for alpha is 0.262, with a f1_score of 79.2%\n",
      "\n",
      "\u001b[1m\u001b[4mLogistic Regression\u001b[0m\n",
      "Where C = 0.010, the sum of squared weights are [2.541 2.940 2.862 2.250]\n",
      "Where C = 0.020, the sum of squared weights are [5.471 5.876 5.941 4.668]\n",
      "Where C = 0.030, the sum of squared weights are [8.399 8.619 8.926 7.098]\n",
      "Where C = 0.040, the sum of squared weights are [11.269 11.206 11.810 9.497]\n",
      "Where C = 0.050, the sum of squared weights are [14.073 13.671 14.605 11.861]\n",
      "Where C = 0.060, the sum of squared weights are [16.804 16.026 17.314 14.176]\n",
      "Where C = 0.070, the sum of squared weights are [19.477 18.304 19.950 16.454]\n",
      "Where C = 0.080, the sum of squared weights are [22.086 20.482 22.516 18.685]\n",
      "Where C = 0.090, the sum of squared weights are [24.630 22.604 25.017 20.877]\n",
      "Where C = 0.100, the sum of squared weights are [27.136 24.660 27.459 23.026]\n",
      "Where C = 0.150, the sum of squared weights are [38.905 34.186 38.906 33.234]\n",
      "Where C = 0.200, the sum of squared weights are [49.733 42.750 49.328 42.671]\n",
      "Where C = 0.250, the sum of squared weights are [59.823 50.562 58.933 51.474]\n",
      "Where C = 0.300, the sum of squared weights are [69.294 57.857 67.907 59.760]\n",
      "Where C = 0.350, the sum of squared weights are [78.248 64.689 76.307 67.593]\n",
      "Where C = 0.400, the sum of squared weights are [86.734 71.146 84.265 75.055]\n",
      "Where C = 0.450, the sum of squared weights are [94.856 77.261 91.822 82.198]\n",
      "Where C = 0.500, the sum of squared weights are [102.622 83.093 99.034 88.951]\n",
      "Where C = 0.550, the sum of squared weights are [110.108 88.666 105.936 95.543]\n",
      "Where C = 0.600, the sum of squared weights are [117.234 94.014 112.521 101.847]\n",
      "Where C = 0.650, the sum of squared weights are [124.150 99.171 118.849 107.940]\n",
      "Where C = 0.700, the sum of squared weights are [130.908 104.150 125.027 113.809]\n",
      "Where C = 0.750, the sum of squared weights are [137.358 108.928 130.919 119.490]\n",
      "Where C = 0.800, the sum of squared weights are [143.566 113.572 136.687 125.082]\n",
      "Where C = 0.850, the sum of squared weights are [149.689 118.098 142.228 130.443]\n",
      "Where C = 0.900, the sum of squared weights are [155.621 122.543 147.642 135.682]\n",
      "Where C = 0.950, the sum of squared weights are [161.393 126.718 152.852 140.814]\n",
      "Where C = 1.000, the sum of squared weights are [167.011 130.941 157.915 145.747]\n",
      "Where C = 2.000, the sum of squared weights are [257.632 197.944 239.994 226.616]\n",
      "Where C = 3.000, the sum of squared weights are [324.022 247.617 300.046 286.878]\n",
      "Where C = 4.000, the sum of squared weights are [377.634 288.164 348.920 335.928]\n",
      "Where C = 5.000, the sum of squared weights are [422.850 322.082 389.632 378.151]\n",
      "Where C = 6.000, the sum of squared weights are [462.929 352.343 425.968 414.951]\n",
      "Where C = 7.000, the sum of squared weights are [498.062 380.489 458.647 447.702]\n",
      "Where C = 8.000, the sum of squared weights are [529.498 405.211 487.819 477.955]\n",
      "Where C = 9.000, the sum of squared weights are [559.827 427.307 514.072 505.167]\n",
      "\n",
      "The best choice for C is 0.550, with a f1_score of 71.2%\n"
     ]
    }
   ],
   "source": [
    "def P3():\n",
    "    ### STUDENT START ###\n",
    "    #Intializes a CountVectorizer for Dev_Data\n",
    "    CV3 = CountVectorizer()\n",
    "    #Transfroms the train_data\n",
    "    vec_train_data = CV3.fit_transform(train_data)\n",
    "    #Transforms the dev_data\n",
    "    vec_dev_data = CV3.transform(dev_data)\n",
    "\n",
    "    Ks = list(range(1,101))\n",
    "    best_k = 0\n",
    "    best_k_score = 0\n",
    "    #Loops through a list of Ks 1 to 100\n",
    "    for i in Ks:\n",
    "        #Builds the nearest neighbor classifier\n",
    "        KN = KNeighborsClassifier(n_neighbors=i)\n",
    "        #Fits the nearest neighbor\n",
    "        KN.fit(vec_train_data,train_labels)\n",
    "        #Gets the f1 score\n",
    "        score = metrics.f1_score(dev_labels,KN.predict(vec_dev_data),average='weighted')\n",
    "        #If this f1 score is better than the current best f1 score, update the best f1 score\n",
    "        if score > best_k_score:\n",
    "            best_k = i\n",
    "            best_k_score = score\n",
    "\n",
    "    print('\\n\\033[1m\\033[4mK-Nearest Neighbors\\033[0m')\n",
    "    print('The best choice for k is {0:.0f}, with a f1_score of {1:.1f}%'.format(best_k,best_k_score*100))\n",
    "\n",
    "    print('\\n\\033[1m\\033[4mMultinomial Naive Bayes\\033[0m')\n",
    "    best_a = 0\n",
    "    best_a_score = 0\n",
    "    alphas = list(np.arange(0,1,0.001))\n",
    "    #loop through the alphas 0 to 1 at 0.001 intervals\n",
    "    for i in alphas:\n",
    "        #Initializes the MNB using the alpha = i\n",
    "        MNB = MultinomialNB(alpha = i)\n",
    "        #Fits the model\n",
    "        MNB.fit(vec_train_data, train_labels)\n",
    "        #Finds the f1 score\n",
    "        score = metrics.f1_score(dev_labels, MNB.predict(vec_dev_data), average='weighted')\n",
    "\n",
    "        #If this f1 score is better than the current best f1 score, update the best f1 score\n",
    "        if score > best_a_score:\n",
    "            best_a_score = score\n",
    "            best_a = i\n",
    "    print('The best choice for alpha is {0:.3f}, with a f1_score of {1:.1f}%'.format(best_a,best_a_score*100))\n",
    "\n",
    "    print('\\n\\033[1m\\033[4mLogistic Regression\\033[0m')\n",
    "    best_c = 0\n",
    "    best_c_score = 0\n",
    "    Cs = list(np.arange(0.01,0.1,0.01))+list(np.arange(0.1,1,0.05))+list(np.arange(1,10,1))\n",
    "    for i in Cs:\n",
    "        #Initializes the LR using the C = i and l2 regularization\n",
    "        lr  = LogisticRegression(C=i,penalty='l2')\n",
    "        #Fit the model\n",
    "        lr.fit(vec_train_data,train_labels)\n",
    "        #Calculates the sum of squared weights\n",
    "        x = np.sum(lr.coef_**2,axis=1)\n",
    "        print('Where C = {0:.3f}, the sum of squared weights are [{1:.3f} {2:.3f} {3:.3f} {4:.3f}]'.\n",
    "              format(i,x[0],x[1],x[2],x[3]))\n",
    "        #Calculates the f1 score\n",
    "        score = metrics.f1_score(dev_labels, lr.predict(vec_dev_data), average='weighted')\n",
    "\n",
    "        # Finding/Replacing the best parameters:\n",
    "        if score > best_c_score:\n",
    "            best_c_score = score\n",
    "            best_c = i\n",
    "    print('')\n",
    "    print('The best choice for C is {0:.3f}, with a f1_score of {1:.1f}%'.format(best_c,best_c_score*100))\n",
    "    ### STUDENT END ###\n",
    "\n",
    "P3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANSWER:\n",
    "- A) The nearest neighbors doesn't work well for this problem because of the vocabulary size. Nearest neighbor classification often suffer for high demensionality problems, which is what we have in this problem. Using such a high k value results in us overfitting the training data, which is why we have such a low best f1 score when applying the model to the dev data.\n",
    "- B) The Naive Bayes model generally works better for smaller datasets because it is generated from a join density function based on the assumption that the features are conditionally independent, which assigns a low but non-zero probability when the dev data has features not previously seen in the training data. The logistic method tends to overfit in smaller datasets as it tries to minimize the error function, which may not work well when the dev set has features not seen in the training data.\n",
    "- C) We can see from our examples that as we increase C, we increase the sum of squared weights. A higher value of C increase the penalty for overfitting the model, which means that a large portion of the outliers will not be captured by the model. This will increase the unexplained variability of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Train a logistic regression model. Find the 5 features with the largest weights for each label -- 20 features in total. Create a table with 20 rows and 4 columns that shows the weight for each of these features for each of the labels. Create the table again with bigram features. Any surprising features in this table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[4mUnigram Features\u001b[0m\n",
      "            alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "atheists       0.860338      -0.094164  -0.274634           -0.663167\n",
      "bobby          0.845821      -0.195499  -0.293898           -0.397430\n",
      "religion       0.818974      -0.523268  -0.676525           -0.053145\n",
      "atheism        0.814398      -0.359795  -0.378286           -0.384411\n",
      "deletion       0.769615      -0.266052  -0.289402           -0.284766\n",
      "graphics      -0.656295       1.667386  -1.133658           -0.643781\n",
      "image         -0.484338       1.148527  -0.692842           -0.392904\n",
      "file          -0.282719       1.087761  -0.699333           -0.521358\n",
      "3d            -0.311020       0.966801  -0.588346           -0.325849\n",
      "computer       0.080976       0.862350  -0.584809           -0.407784\n",
      "space         -1.088026      -1.142998   1.903677           -0.998101\n",
      "orbit         -0.358880      -0.574347   1.040770           -0.516125\n",
      "nasa          -0.479408      -0.420247   0.872742           -0.410438\n",
      "launch        -0.389762      -0.406040   0.804192           -0.287973\n",
      "spacecraft    -0.305347      -0.331514   0.761407           -0.303251\n",
      "christians    -0.625324      -0.327803  -0.419670            0.958323\n",
      "christian     -0.499196      -0.349391  -0.250337            0.950954\n",
      "blood         -0.443026      -0.098572  -0.241505            0.874559\n",
      "fbi           -0.252672      -0.224962  -0.384805            0.775982\n",
      "order         -0.668913      -0.067251  -0.127306            0.764069\n",
      "\n",
      "\u001b[1m\u001b[4mBigram Features\u001b[0m\n",
      "               alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "claim that        0.628117      -0.206460  -0.284266           -0.147676\n",
      "cheers kent       0.569330      -0.721434  -0.684565            0.544933\n",
      "was just          0.507435      -0.138865  -0.137167           -0.236601\n",
      "you are           0.486081      -0.285081  -0.493883            0.024889\n",
      "are you           0.457080      -0.255166  -0.098763           -0.316172\n",
      "looking for      -0.646772       1.136888  -0.514686           -0.588472\n",
      "in advance       -0.470715       0.851666  -0.450674           -0.430072\n",
      "comp graphics    -0.303036       0.832212  -0.383377           -0.298626\n",
      "out there        -0.281344       0.777349  -0.492334           -0.284469\n",
      "is there         -0.352258       0.776604  -0.481371           -0.267552\n",
      "the space        -0.274280      -0.545660   0.892701           -0.280500\n",
      "the moon         -0.357929      -0.504011   0.847280           -0.217362\n",
      "sci space        -0.265753      -0.337124   0.637129           -0.228152\n",
      "and such         -0.209366      -0.345922   0.603880           -0.223422\n",
      "it was           -0.210395      -0.317195   0.537080           -0.323802\n",
      "the fbi          -0.136423      -0.217958  -0.303856            0.565986\n",
      "cheers kent       0.569330      -0.721434  -0.684565            0.544933\n",
      "ignorance is     -0.166306      -0.180740  -0.147824            0.534056\n",
      "but he           -0.198297      -0.224464  -0.142931            0.507798\n",
      "is strength      -0.121681      -0.156150  -0.132820            0.448314\n"
     ]
    }
   ],
   "source": [
    "def make_df(coefficients,feature_names):\n",
    "    ### STUDENT START ###\n",
    "    #Builds a dataframe of the coefficients with the target_names as the columns\n",
    "    df = pd.DataFrame(coefficients,columns=newsgroups_train.target_names)\n",
    "    #Makes the DataFrame index the feature names\n",
    "    df.index = feature_names\n",
    "    #Finds the top 5 features for each of the target names\n",
    "    alt_atheism = df.sort_values('alt.atheism',ascending=False)[:5]\n",
    "    comp_graphics = df.sort_values('comp.graphics',ascending=False)[:5]\n",
    "    sci_space = df.sort_values('sci.space',ascending=False)[:5]\n",
    "    talk_religion_misc = df.sort_values('talk.religion.misc',ascending=False)[:5]\n",
    "    #Builds master data class\n",
    "    total_df = alt_atheism.append(comp_graphics).append(sci_space).append(talk_religion_misc)\n",
    "    return total_df \n",
    "    ### STUDENT END ###\n",
    "\n",
    "def P4():\n",
    "    ### STUDENT START ###\n",
    "    #Unigram Feature Section \n",
    "    print('\\n\\033[1m\\033[4mUnigram Features\\033[0m')\n",
    "    #Intializes a CountVectorizer for Dev_Data\n",
    "    CV4_uni = CountVectorizer()\n",
    "    #Transfroms the train_data\n",
    "    vec_train_data_uni = CV4_uni.fit_transform(train_data)\n",
    "    #Transforms the dev_data\n",
    "    vec_dev_data_uni = CV4_uni.transform(dev_data)\n",
    "    #Initializes the LR using the C = 0.55 (The best C from the last model) and l2 regularization\n",
    "    lr_uni  = LogisticRegression(penalty='l2',C=0.55)\n",
    "    #Fits the LR model to the training data\n",
    "    lr_uni.fit(vec_train_data_uni, train_labels)\n",
    "    #Takes the LR coefficients and transposes them to fit the dataframe. Also passes the feature name. \n",
    "    #A Dataframe is returned\n",
    "    uni_df = make_df(lr_uni.coef_.T,CV4_uni.get_feature_names())\n",
    "    print(uni_df)\n",
    "\n",
    "    #Bigram Feature Section \n",
    "    print('\\n\\033[1m\\033[4mBigram Features\\033[0m')\n",
    "    #Intializes a CountVectorizer for Dev_Data\n",
    "    CV4_bi = CountVectorizer(ngram_range=(2,2))\n",
    "    #Transfroms the train_data\n",
    "    vec_train_data_bi = CV4_bi.fit_transform(train_data)\n",
    "    #Transforms the dev_data\n",
    "    vec_dev_data_bi = CV4_bi.transform(dev_data)\n",
    "    #Initializes the LR using the C = 0.55 (The best C from the last model) and l2 regularization\n",
    "    lr_bi  = LogisticRegression(penalty='l2',C=0.55)\n",
    "    #Fits the LR model to the training data\n",
    "    lr_bi.fit(vec_train_data_bi, train_labels)\n",
    "    #Takes the LR coefficients and transposes them to fit the dataframe. Also passes the feature name. \n",
    "    #A Dataframe is returned\n",
    "    bi_df = make_df(lr_bi.coef_.T,CV4_bi.get_feature_names())\n",
    "    print(bi_df)\n",
    "    ### STUDENT END ###\n",
    "\n",
    "P4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANSWER:\n",
    "In the unigram features, most of the top features are descriptive words which directly relate to the classification labels. \n",
    "\n",
    "In the bigram features \"cheers kent\" appears in two target labels, alt.atheism and talk.religion.misc. This is suprising as we would expect any shared phrases to be common words. Along those lines several of the other bigram features are combinations of common words such as \"it was\" or \"is there\". In fact, two of the features with the largest weights under the alt.atheism label are just reversals of two common words, \"you are\" and \"are you\". Also worth mentioning is that the largest weights are lower than they were for the unigram features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Try to improve the logistic regression classifier by passing a custom preprocessor to CountVectorizer. The preprocessing function runs on the raw text, before it is split into words by the tokenizer. Your preprocessor should try to normalize the input in various ways to improve generalization. For example, try lowercasing everything, replacing sequences of numbers with a single token, removing various other non-letter characters, and shortening long words. If you're not already familiar with regular expressions for manipulating strings, see https://docs.python.org/2/library/re.html, and re.sub() in particular. With your new preprocessor, how much did you reduce the size of the dictionary?\n",
    "\n",
    "For reference, I was able to improve dev F1 by 2 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with no Preprocessing: 70.37%\n",
      "F1 Score with Preprocessing: 71.30%\n",
      "\n",
      "The length of the features without preprocessing is 33,291\n",
      "The length of the features after preprocessing is 21,851\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "def better_preprocessor(s):\n",
    "    ### STUDENT START ###\n",
    "    #Change encoding\n",
    "    s  = s.encode('utf-8')\n",
    "    #Makes all strings lowercase\n",
    "    s = s.lower()\n",
    "    #Remove punctuation\n",
    "    s = str(s).translate(None,string.punctuation)\n",
    "    #Replaces all numbers with -99\n",
    "    s = re.sub('((0.)?([0-9]+))',r'-99',s)\n",
    "    #For determining the optimal word length cutoff, I have referenced: http://norvig.com/mayzner.html\n",
    "    #According to this:\n",
    "            #\"And here is the breakdown of mentions (in millions) by word length (looking like a Poisson distribution). \n",
    "            #The average is 4.79 letters per word, and 80% are between 2 and 7 letters long.\"\n",
    "    #As a result we will trim all words over 7 letters long to 7 letters\n",
    "    #Found  ReGex logic https://stackoverflow.com/questions/6853698/regex-to-truncate-long-words-and-append-ellipses\n",
    "    s = re.sub('(?<=(\\s\\w{7}))(\\w*)','',s)\n",
    "    return s\n",
    "    ### STUDENT END ###\n",
    "    \n",
    "\n",
    "def P5():\n",
    "    ### STUDENT START ###\n",
    "    #Trains the Count Vectorizer with no prepocessing\n",
    "    cv_empt = CountVectorizer(preprocessor=empty_preprocessor)\n",
    "    #Transforms the training data\n",
    "    train_empt = cv_empt.fit_transform(train_data)\n",
    "    #Transforms the dev data using the training data transform\n",
    "    dev_empt = cv_empt.transform(dev_data)\n",
    "    #Initializes the LR using the C = 0.55 (The best C from p3) and l2 regularization\n",
    "    lr_empt  = LogisticRegression(penalty='l2',C=0.55)\n",
    "    #Fits the LR model to the training data\n",
    "    lr_empt.fit(train_empt, train_labels)\n",
    "    print('F1 Score with no Preprocessing: {0:.2f}%'.format(metrics.f1_score(dev_labels,\n",
    "                                                              lr_empt.predict(dev_empt),average='weighted')*100))\n",
    "\n",
    "    #Trains the Count Vectorizer with prepocessing\n",
    "    cv_pre = CountVectorizer(preprocessor=better_preprocessor)\n",
    "    #Transforms the training data using the preprocessing fit transform\n",
    "    train_pre = cv_pre.fit_transform(train_data)\n",
    "    #Transforms the dev data using the preprocessing training data transform\n",
    "    dev_pre = cv_pre.transform(dev_data)\n",
    "    #Initializes the LR using the C = 0.55 (The best C from p3) and l2 regularization\n",
    "    lr_pre = LogisticRegression(penalty='l2',C=0.55)\n",
    "    #Fits the LR model to the training data\n",
    "    lr_pre.fit(train_pre, train_labels)   \n",
    "    print('F1 Score with Preprocessing: {0:.2f}%'.format(metrics.f1_score(dev_labels,\n",
    "                                                              lr_pre.predict(dev_pre),average='weighted')*100))\n",
    "\n",
    "\n",
    "    print('')\n",
    "    print('The length of the features without preprocessing is {0:,.0f}'.format(len(cv_empt.get_feature_names())))\n",
    "    print('The length of the features after preprocessing is {0:,.0f}'.format(len(cv_pre.get_feature_names())))\n",
    "    ### STUDENT END ###\n",
    "\n",
    "P5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) The idea of regularization is to avoid learning very large weights (which are likely to fit the training data, but not generalize well) by adding a penalty to the total size of the learned weights. That is, logistic regression seeks the set of weights that minimizes errors in the training data AND has a small size. The default regularization, L2, computes this size as the sum of the squared weights (see P3, above). L1 regularization computes this size as the sum of the absolute values of the weights. The result is that whereas L2 regularization makes all the weights relatively small, L1 regularization drives lots of the weights to 0, effectively removing unimportant features.\n",
    "\n",
    "#### Train a logistic regression model using a \"l1\" penalty. Output the number of learned weights that are not equal to zero. How does this compare to the number of non-zero weights you get with \"l2\"? Now, reduce the size of the vocabulary by keeping only those features that have at least one non-zero weight and retrain a model using \"l2\".\n",
    "\n",
    "#### Make a plot showing accuracy of the re-trained model vs. the vocabulary size you get when pruning unused features by adjusting the C parameter.\n",
    "\n",
    "##### Note: The gradient descent code that trains the logistic regression model sometimes has trouble converging with extreme settings of the C parameter. Relax the convergence criteria by setting tol=.01 (the default is .0001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of non-zero items using L1 regularization is 1,243\n",
      "The number of non-zero items using L2 regularization is 107,516\n",
      "\n",
      "\u001b[1m\u001b[4mVocab Length and Accuracy for different values of C\u001b[0m\n",
      "Value for C: 0.01 | Vocab Length:    18 | Accuracy: 46.30%\n",
      "Value for C: 0.02 | Vocab Length:    41 | Accuracy: 53.54%\n",
      "Value for C: 0.03 | Vocab Length:    64 | Accuracy: 57.80%\n",
      "Value for C: 0.04 | Vocab Length:    99 | Accuracy: 59.24%\n",
      "Value for C: 0.05 | Vocab Length:   122 | Accuracy: 61.51%\n",
      "Value for C: 0.06 | Vocab Length:   136 | Accuracy: 61.70%\n",
      "Value for C: 0.07 | Vocab Length:   155 | Accuracy: 61.48%\n",
      "Value for C: 0.08 | Vocab Length:   171 | Accuracy: 63.93%\n",
      "Value for C: 0.09 | Vocab Length:   185 | Accuracy: 66.46%\n",
      "Value for C: 0.10 | Vocab Length:   205 | Accuracy: 67.67%\n",
      "Value for C: 0.15 | Vocab Length:   303 | Accuracy: 68.27%\n",
      "Value for C: 0.20 | Vocab Length:   365 | Accuracy: 68.50%\n",
      "Value for C: 0.25 | Vocab Length:   426 | Accuracy: 68.52%\n",
      "Value for C: 0.30 | Vocab Length:   525 | Accuracy: 67.11%\n",
      "Value for C: 0.35 | Vocab Length:   585 | Accuracy: 67.60%\n",
      "Value for C: 0.40 | Vocab Length:   620 | Accuracy: 68.13%\n",
      "Value for C: 0.45 | Vocab Length:   672 | Accuracy: 67.16%\n",
      "Value for C: 0.50 | Vocab Length:   760 | Accuracy: 66.63%\n",
      "Value for C: 0.55 | Vocab Length:   812 | Accuracy: 68.00%\n",
      "Value for C: 0.60 | Vocab Length:   842 | Accuracy: 68.39%\n",
      "Value for C: 0.65 | Vocab Length:   872 | Accuracy: 67.08%\n",
      "Value for C: 0.70 | Vocab Length:   901 | Accuracy: 66.79%\n",
      "Value for C: 0.75 | Vocab Length:   924 | Accuracy: 67.78%\n",
      "Value for C: 0.80 | Vocab Length:   953 | Accuracy: 67.01%\n",
      "Value for C: 0.85 | Vocab Length: 1,019 | Accuracy: 67.51%\n",
      "Value for C: 0.90 | Vocab Length: 1,058 | Accuracy: 67.27%\n",
      "Value for C: 0.95 | Vocab Length: 1,083 | Accuracy: 68.00%\n",
      "Value for C: 1.00 | Vocab Length: 1,052 | Accuracy: 66.49%\n",
      "Value for C: 2.00 | Vocab Length: 1,488 | Accuracy: 68.03%\n",
      "Value for C: 3.00 | Vocab Length: 1,800 | Accuracy: 68.28%\n",
      "Value for C: 4.00 | Vocab Length: 1,833 | Accuracy: 67.56%\n",
      "Value for C: 5.00 | Vocab Length: 2,249 | Accuracy: 67.06%\n",
      "Value for C: 6.00 | Vocab Length: 2,494 | Accuracy: 67.07%\n",
      "Value for C: 7.00 | Vocab Length: 2,601 | Accuracy: 69.31%\n",
      "Value for C: 8.00 | Vocab Length: 2,422 | Accuracy: 69.68%\n",
      "Value for C: 9.00 | Vocab Length: 2,549 | Accuracy: 69.33%\n",
      "\n",
      "\u001b[1m\u001b[4mPlotting Accuracy vs Vocab Size\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118aa93d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model_C_Adjustment(CV6,c_var,vec_train_data,vec_dev_data):\n",
    "    #Trains the logistic regression model using the C value of c_var\n",
    "    lr_temp = LogisticRegression(penalty='l1',C=c_var,tol=0.01)\n",
    "    lr_temp.fit(vec_train_data,train_labels)\n",
    "    #Convert it to a dataframe to make it easier to work with\n",
    "    df = pd.DataFrame(lr_temp.coef_).T\n",
    "    df.index = CV6.get_feature_names()\n",
    "    #Find the non_zero values\n",
    "    df = df[df.sum(1) <> 0]\n",
    "    #Count vectorizer with vocabulary set using the new vocab list of non-zeros\n",
    "    cv_reformat = CountVectorizer(vocabulary=df.index)\n",
    "    vec_train_reformat = cv_reformat.fit_transform(train_data)\n",
    "    vec_dev_reformat = cv_reformat.transform(dev_data)\n",
    "    #Retrains the LR model after trimming the vocabulary\n",
    "    lr_reformat = LogisticRegression(penalty='l2',C=c_var,tol=0.01)\n",
    "    lr_reformat.fit(vec_train_reformat, train_labels)\n",
    "    return {len(df):metrics.f1_score(dev_labels,lr_reformat.predict(vec_dev_reformat),average='weighted')}\n",
    "    \n",
    "def P6():\n",
    "    # Keep this random seed here to make comparison easier.\n",
    "    np.random.seed(0)\n",
    "    ### STUDENT START ###\n",
    "    CV6 = CountVectorizer()\n",
    "    vec_train_data = CV6.fit_transform(train_data)\n",
    "    vec_dev_data = CV6.transform(dev_data)\n",
    "    #Initializes the LR using the C = 0.55 (The best C from p3) and l1 regularization\n",
    "    lr_l1 = LogisticRegression(C=0.55,penalty='l1')\n",
    "    lr_l1.fit(vec_train_data,train_labels)\n",
    "    print('The number of non-zero items using L1 regularization is {0:,.0f}'.format(np.count_nonzero(lr_l1.coef_)))\n",
    "    #Initializes the LR using the C = 0.55 (The best C from p3) and l2 regularization\n",
    "    lr_l2 = LogisticRegression(C=0.55,penalty='l2')\n",
    "    lr_l2.fit(vec_train_data,train_labels)\n",
    "    print('The number of non-zero items using L2 regularization is {0:,.0f}'.format(np.count_nonzero(lr_l2.coef_)))\n",
    "    \n",
    "    #Create a list of Cs to iterate through\n",
    "    Cs = list(np.arange(0.01,0.1,0.01))+list(np.arange(0.1,1,0.05))+list(np.arange(1,10,1))\n",
    "    Values = {}\n",
    "    print('\\n\\033[1m\\033[4mVocab Length and Accuracy for different values of C\\033[0m')\n",
    "    for c in Cs:\n",
    "        temp_dict = model_C_Adjustment(CV6,c,vec_train_data,vec_dev_data)\n",
    "        Values.update(temp_dict)\n",
    "        print('Value for C: {0:2.2f} | Vocab Length: {1:5,d} | Accuracy: {2:2.2f}%'\n",
    "              .format(c,temp_dict.keys()[0],temp_dict.values()[0]*100))\n",
    "            \n",
    "    print('\\n\\033[1m\\033[4mPlotting Accuracy vs Vocab Size\\033[0m') \n",
    "    #Plots the results    \n",
    "    plt.scatter(Values.keys(),Values.values());\n",
    "    plt.xlabel('Vocabulary Size');\n",
    "    plt.ylabel('Accuracy');\n",
    "    ### STUDENT END ###\n",
    "\n",
    "P6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) Use the TfidfVectorizer -- how is this different from the CountVectorizer? Train a logistic regression model with C=100.\n",
    "\n",
    "#### Make predictions on the dev data and show the top 3 documents where the ratio R is largest, where R is:\n",
    "\n",
    "##### maximum predicted probability / predicted probability of the correct label\n",
    "\n",
    "#### What kinds of mistakes is the model making? Suggest a way to address one particular issue that you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logistic model with a C=100 and feature vectors constructed with the TfidfVectorizer is 75.98%\n",
      "\n",
      "\u001b[1m\u001b[4mThe largest R value is 929.36\u001b[0m\n",
      "\n",
      "\u001b[1mThe predicted label for this article is comp.graphics\u001b[0m\n",
      "\u001b[1mThe actual label for this article is talk.religion.misc\u001b[0m\n",
      "I am pleased to announce that a *revised version* of _The Easy-to-Read Book\n",
      "of Mormon_ (former title: _Mormon's Book_) by Lynn Matthews Anderson is now\n",
      "available through anonymous ftp (see information below). In addition to the\n",
      "change in title, the revised ETR BOM has been shortened by several pages\n",
      "(eliminating many extraneous \"that's\" and \"of's\"), and many (minor) errors\n",
      "have been corrected. This release includes a simplified Joseph Smith Story,\n",
      "testimonies of the three and eight witnesses, and a \"Words-to-Know\"\n",
      "glossary.\n",
      "\n",
      "As with the previous announcement, readers are reminded that this is a\n",
      "not-for-profit endeavor. This is a copyrighted work, but people are welcome\n",
      "to make *verbatim* copies for personal use. People can recuperate the\n",
      "actual costs of printing (paper, copy center charges), but may not charge\n",
      "anything for their time in making copies, or in any way realize a profit\n",
      "from the use of this book. See the permissions notice in the book itself\n",
      "for the precise terms.\n",
      "\n",
      "Negotiations are currently underway with a Mormon publisher vis-a-vis the\n",
      "printing and distribution of bound books. (Sorry, I'm out of the wire-bound\n",
      "\"first editions.\") I will make another announcement about the availability\n",
      "of printed copies once everything has been worked out.\n",
      "\n",
      "FTP information: connect via anonymous ftp to carnot.itc.cmu.edu, then \"cd\n",
      "pub\" (you won't see anything at all until you do).\n",
      "\n",
      "\"The Easy-to-Read Book of Mormon\" is currently available in postscript and\n",
      "RTF (rich text format). (ASCII, LaTeX, and other versions can be made\n",
      "available; contact dba@andrew.cmu.edu for details.) You should be able to\n",
      "print the postscript file on any postscript printer (such as an Apple\n",
      "Laserwriter); let dba know if you have any difficulties. (The postscript in\n",
      "the last release had problems on some printers; this time it should work\n",
      "better.) RTF is a standard document interchange format that can be read in\n",
      "by a number of word processors, including Microsoft Word for both the\n",
      "Macintosh and Windows. If you don't have a postscript printer, you may be\n",
      "able to use the RTF file to print out a copy of the book.\n",
      "\n",
      "-r--r--r--  1 dba                   1984742 Apr 27 13:12 etrbom.ps\n",
      "-r--r--r--  1 dba                   1209071 Apr 27 13:13 etrbom.rtf\n",
      "\n",
      "For more information about how this project came about, please refer to my\n",
      "article in the current issue of _Sunstone_, entitled \"Delighting in\n",
      "Plainness: Issues Surrounding a Simple Modern English Book of Mormon.\"\n",
      "\n",
      "Send all inquiries and comments to:\n",
      "\n",
      "    Lynn Matthews Anderson\n",
      "    5806 Hampton Street\n",
      "    Pittsburgh, PA 15206\n",
      "\n",
      "\u001b[1mThe top 10 features used to make the false prediction for this article are:\u001b[0m\n",
      "         alt.atheism  comp.graphics  sci.space  talk.religion.misc  \\\n",
      "file       -2.456895       8.884874  -5.711058           -4.674778   \n",
      "with       -4.619076       5.267230  -2.592169           -0.795171   \n",
      "format     -2.658531       5.247354  -3.929393           -2.066159   \n",
      "number     -2.622742       5.043114  -3.409201           -0.063189   \n",
      "version    -0.979233       4.910954  -3.025792           -1.271534   \n",
      "windows    -2.219344       4.571159  -2.897102           -1.850371   \n",
      "any        -0.312050       4.166309  -2.336357           -4.339462   \n",
      "work       -1.624995       4.119211  -0.983956           -1.965890   \n",
      "use        -3.649912       3.973410  -0.151372           -3.034202   \n",
      "ftp        -2.670633       3.825057  -2.826161           -2.884670   \n",
      "\n",
      "         % of Train Articles  \n",
      "file                0.047198  \n",
      "with                0.373648  \n",
      "format              0.028515  \n",
      "number              0.056047  \n",
      "version             0.042281  \n",
      "windows             0.025565  \n",
      "any                 0.237463  \n",
      "work                0.083088  \n",
      "use                 0.129302  \n",
      "ftp                 0.035890  \n",
      "\n",
      "\u001b[1m\u001b[4mThe 2nd largest R value is 325.00\u001b[0m\n",
      "\n",
      "\u001b[1mThe predicted label for this article is comp.graphics\u001b[0m\n",
      "\u001b[1mThe actual label for this article is talk.religion.misc\u001b[0m\n",
      "Can anyone provide me a ftp site where I can obtain a online version\n",
      "of the Book of Mormon. Please email the internet address if possible.\n",
      "\n",
      "\u001b[1mThe top 10 features used to make the false prediction for this article are:\u001b[0m\n",
      "         alt.atheism  comp.graphics  sci.space  talk.religion.misc  \\\n",
      "version    -0.979233       4.910954  -3.025792           -1.271534   \n",
      "site       -1.566059       4.783325  -2.414924           -1.870577   \n",
      "anyone     -1.904236       4.535399  -1.140739           -5.532378   \n",
      "ftp        -2.670633       3.825057  -2.826161           -2.884670   \n",
      "email      -2.987330       2.662542  -0.435634           -1.019298   \n",
      "address    -0.352629       2.558054  -1.597593           -1.062369   \n",
      "where      -2.714303       2.150511  -0.192727           -0.825496   \n",
      "book        2.448956       1.792581  -0.996359           -2.948120   \n",
      "can        -0.524247       1.760191   0.210016           -4.335901   \n",
      "obtain     -0.461793       1.324351  -1.194637           -0.452790   \n",
      "\n",
      "         % of Train Articles  \n",
      "version             0.042281  \n",
      "site                0.032448  \n",
      "anyone              0.121436  \n",
      "ftp                 0.035890  \n",
      "email               0.039331  \n",
      "address             0.037856  \n",
      "where               0.126844  \n",
      "book                0.052114  \n",
      "can                 0.334808  \n",
      "obtain              0.008358  \n",
      "\n",
      "\u001b[1m\u001b[4mThe 3rd largest R value is 287.18\u001b[0m\n",
      "\n",
      "\u001b[1mThe predicted label for this article is talk.religion.misc\u001b[0m\n",
      "\u001b[1mThe actual label for this article is alt.atheism\u001b[0m\n",
      "\n",
      "The 24 children were, of course, killed by a lone gunman in a second story\n",
      "window, who fired eight bullets in the space of two seconds...\n",
      "\n",
      "\n",
      "\u001b[1mThe top 10 features used to make the false prediction for this article are:\u001b[0m\n",
      "          alt.atheism  comp.graphics  sci.space  talk.religion.misc  \\\n",
      "children    -4.308099      -1.700178  -1.642072            5.641998   \n",
      "story       -1.350145      -2.207586  -1.033349            4.792206   \n",
      "who          1.461924      -4.869092  -2.344325            3.854513   \n",
      "were         0.407291      -2.478108  -2.236054            2.940514   \n",
      "two         -4.169629       1.706995  -1.550793            2.889732   \n",
      "second      -3.094186       0.366474  -0.783194            2.827145   \n",
      "by          -1.872232      -0.835632  -1.252394            1.546026   \n",
      "killed       0.391189      -1.123190  -0.976630            1.338653   \n",
      "of           0.367341      -3.993977  -1.150389            0.789702   \n",
      "fired       -0.038214      -0.108124  -0.139523            0.464217   \n",
      "\n",
      "          % of Train Articles  \n",
      "children             0.026549  \n",
      "story                0.020649  \n",
      "who                  0.196657  \n",
      "were                 0.161750  \n",
      "two                  0.097837  \n",
      "second               0.043265  \n",
      "by                   0.300885  \n",
      "killed               0.015733  \n",
      "of                   0.707965  \n",
      "fired                0.002950  \n"
     ]
    }
   ],
   "source": [
    "def GetColFromLabel(row):\n",
    "    ### STUDENTSTART ###\n",
    "    #Function for use in the DF.Apply to get the column value of a label\n",
    "    return row[int(row.Labels)]\n",
    "    ## STUDENT END ###\n",
    "\n",
    "def P7():\n",
    "    ### STUDENTSTART ###\n",
    "    #Train the TFID Vectorizer\n",
    "    TV = TfidfVectorizer()\n",
    "    #Fit the training and dev data\n",
    "    tv_train_data = TV.fit_transform(train_data)\n",
    "    tv_dev_data = TV.transform(dev_data)\n",
    "\n",
    "    #Train the logistic regression model using the L2 Penalty and a C of 100\n",
    "    lr_tv = LogisticRegression(penalty='l2',C=100)\n",
    "    lr_tv.fit(tv_train_data,train_labels)\n",
    "    #Determine the predicted probabilities\n",
    "    tv_prob = lr_tv.predict_proba(tv_dev_data)\n",
    "    #Prints the F1 Score\n",
    "    print('The logistic model with a C=100 and feature vectors constructed with the TfidfVectorizer is {0:0.2f}%'\n",
    "          .format(metrics.f1_score(dev_labels, lr_tv.predict(tv_dev_data), average='weighted')*100))\n",
    "    #Creates dataframe for easier useage\n",
    "    df = pd.DataFrame(tv_prob)\n",
    "    #Find the max probaility of each example\n",
    "    df['Max'] = df.max(axis=1)\n",
    "    #Add the Labels to the DataFrame\n",
    "    df['Labels'] = pd.Series(dev_labels)\n",
    "    #Find the Probability of the Actual Label\n",
    "    df['LabelProbability'] = df.apply(GetColFromLabel,axis=1)\n",
    "    #Calculate the R Value\n",
    "    df['R'] = np.round(df['Max']/df['LabelProbability'],2)\n",
    "    #Find the Max Value\n",
    "    df= df.sort_values('R',ascending=False)\n",
    "\n",
    "    #Makes a label index for use in examining top features\n",
    "    label_df = pd.DataFrame(lr_tv.coef_).T\n",
    "    label_df.index = TV.get_feature_names()\n",
    "\n",
    "    #Loop through the top 3 values\n",
    "    for i,x in enumerate(df.iloc[0:3].index.values):\n",
    "        StrVals = {1:'largest',2:'2nd largest',3:'3rd largest'}\n",
    "        #Prints the results\n",
    "        print('\\n\\033[1m\\033[4mThe {0} R value is {1:0.2f}\\033[0m'.format(StrVals[i+1],df.loc[x,'R'])) \n",
    "        print('')\n",
    "        print('\\033[1mThe predicted label for this article is {}\\033[0m'\n",
    "              .format(newsgroups_train.target_names[df[[0,1,2,3]].idxmax(axis=1)[x]]))\n",
    "        print('\\033[1mThe actual label for this article is {}\\033[0m'.\n",
    "              format(newsgroups_train.target_names[df.loc[x,'Labels']]))\n",
    "\n",
    "        print(dev_data[x])\n",
    "\n",
    "        #Finds the top features for this bad prediction\n",
    "        print('')\n",
    "        print('\\033[1mThe top 10 features used to make the false prediction for this article are:\\033[0m')\n",
    "        #Finds the non_zero features\n",
    "        temp_list = list(np.nonzero(tv_dev_data[x])[1])\n",
    "        Max_DF = None\n",
    "        #Loops through the non_zero features and finds the score\n",
    "        for j in temp_list:\n",
    "            if Max_DF is None:\n",
    "                Max_DF = label_df[label_df.index == TV.get_feature_names()[j]]\n",
    "                Max_DF.is_copy= False\n",
    "                #Calculates the frequency of occrance in the training data\n",
    "                Max_DF['% of Train Articles'] = len(np.nonzero(tv_train_data[:,j])[0])*1.0/tv_train_data.shape[0]\n",
    "            else:\n",
    "                temp_label_df=label_df[label_df.index == TV.get_feature_names()[j]]\n",
    "                temp_label_df.is_copy= False\n",
    "                #Calculates the frequency of occrance in the training data\n",
    "                temp_label_df['% of Train Articles'] = len(np.nonzero(tv_train_data[:,j])[0])*1.0/tv_train_data.shape[0]\n",
    "                Max_DF = Max_DF.append(temp_label_df) \n",
    "        max_label = df[[0,1,2,3]].idxmax(axis=1)[x]\n",
    "        Max_DF = Max_DF.sort_values(max_label,ascending=False).head(10)\n",
    "        Max_DF = Max_DF.rename(columns={i:name for i,name in enumerate(newsgroups_train.target_names)})\n",
    "        print(Max_DF)\n",
    "    ## STUDENT END ###\n",
    "\n",
    "P7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANSWER:\n",
    "CountVectorizer is simple count of word frequencies while TfidfVectorizer uses a Term-Frequency*Inverse Document-Frequency adjustment. In the TfidfVectorizer: $tf(t,d)*\\log\\frac{1+n_d}{1+df(d,t)} +1$, where tf(t,d) is the number of occurences of term t in a given document d, $n_d$ is the number of documnets, and df(d,t) is the number of documents containing term t. Common stop words carry very little meaningful information for classifcation, but can mask the signifance of more relavant words, which the TfidVectorizer tries to account for by reducing the factor weighting for these frequent terms.\n",
    "\n",
    "One noteable thing, is that the 2 largest R values, carry a similar mistake in that they both contain words commonly associated with tech, for example FTP occurs in both the mis-labels. FTP occurs in just 3.6% of the training data, so perhaps it would be useful to exclude some of these rarely used words by coming up with a word usage cutoff that the word must appread in atleast 5% of all articles. \n",
    "\n",
    "Additionally, and perhaps more concerning, is that the top 10 features in all 3 articles are heavily made of stop words. Even after the TFID adjustment, these frequently used words are still heavily weighting the labeling of these articles with poor R ratios, which is likely still masking important features like Mormon. One future model improvement would be to take advantage of the TfidfVectorizer's ability to remove stop words. We did not make that adjustment here as the default is none."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) EXTRA CREDIT\n",
    "#### Try implementing one of your ideas based on your error analysis. Use logistic regression as your underlying model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
